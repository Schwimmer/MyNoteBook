{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 语料库和向量空间\n",
    "\n",
    "首先是环境配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder \"/tmp\" will be used to save temporary dictionary and corpus.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "TEMP_FOLDER = tempfile.gettempdir()\n",
    "print('Folder \"{}\" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 字符串转为词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",              \n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "             \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看做一个小的语料集，每行是一个doc，一共9篇\n",
    "\n",
    "1、对documents做预处理，包括去掉停用词，删除仅出现一次的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "#停用词\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [ [word for word in document.lower().split() if word not in stoplist ]\n",
    "         for document in documents ]\n",
    "\n",
    "#删除仅出现一次的词\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "texts = [[token for token in text if frequency[token] > 1 ] for text in texts]\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将其转成词袋模型(bag-of-words）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-06 16:45:04,884 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-11-06 16:45:04,885 : INFO : built Dictionary(12 unique tokens: [u'minors', u'graph', u'system', u'trees', u'eps']...) from 9 documents (total 29 corpus positions)\n",
      "2017-11-06 16:45:04,888 : INFO : saving Dictionary object under /tmp/deerwester.dict, separately None\n",
      "2017-11-06 16:45:04,890 : INFO : saved /tmp/deerwester.dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: [u'minors', u'graph', u'system', u'trees', u'eps']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "#词典保存到本地\n",
    "dictionary.save(os.path.join(TEMP_FOLDER, 'deerwester.dict'))\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，通过gemsim.corpora.dictionary.Dictionary类，给每个单词一个id，同时也收集词频等相关统计。这里能看到所有文档共出现了12个单词。用dictionary.token2id可以看到每个单词的编号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'minors': 11, u'graph': 10, u'system': 6, u'trees': 9, u'eps': 8, u'computer': 1, u'survey': 5, u'user': 7, u'human': 2, u'time': 4, u'interface': 0, u'response': 3}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再来一篇文档，看一下转成词袋模型的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (2, 1)]\n"
     ]
    }
   ],
   "source": [
    "new_doc = \"Human computer interaction\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "pprint(new_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，interaction没有在语料库中出现，就没有转成稀疏向量。\n",
    "在scikit-learn中，doc2bow()类似于[CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)类的fit_transform()。\n",
    "\n",
    "接下来，再用训练好的模型将整个语料库转成稀疏向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-06 16:45:04,939 : INFO : storing corpus in Matrix Market format to /tmp/deerwester.mm\n",
      "2017-11-06 16:45:04,941 : INFO : saving sparse matrix to /tmp/deerwester.mm\n",
      "2017-11-06 16:45:04,943 : INFO : PROGRESS: saving document #0\n",
      "2017-11-06 16:45:04,944 : INFO : saved 9x12 matrix, density=25.926% (28/108)\n",
      "2017-11-06 16:45:04,950 : INFO : saving MmCorpus index to /tmp/deerwester.mm.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1)]\n",
      "[(1, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n",
      "[(0, 1), (6, 1), (7, 1), (8, 1)]\n",
      "[(2, 1), (6, 2), (8, 1)]\n",
      "[(3, 1), (4, 1), (7, 1)]\n",
      "[(9, 1)]\n",
      "[(9, 1), (10, 1)]\n",
      "[(9, 1), (10, 1), (11, 1)]\n",
      "[(5, 1), (10, 1), (11, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts] \n",
    "#词袋模型保存到本地\n",
    "corpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'deerwester.mm'), corpus)\n",
    "for c in corpus:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Corpus流处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果训练的语料库很大，不宜都放到内存。 \n",
    "假设现在的文档在磁盘上，一行一个文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyCorpus(object):\n",
    "    def __iter__(self):\n",
    "        for line in open('/home/david/code/jupyter/gensim/mycorpus.txt'):\n",
    "            yield dictionary.doc2bow(line.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MyCorpus object at 0x7f26c05bfc50>\n"
     ]
    }
   ],
   "source": [
    "corpus_memory_friendly = MyCorpus()\n",
    "print(corpus_memory_friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1)]\n",
      "[(1, 1), (3, 1), (5, 1), (6, 1), (7, 1)]\n",
      "[(0, 1), (7, 1), (8, 1)]\n",
      "[(2, 1), (6, 2)]\n",
      "[(3, 1), (4, 1), (7, 1)]\n",
      "[]\n",
      "[(10, 1)]\n",
      "[(9, 1), (10, 1), (11, 1)]\n",
      "[(5, 1), (10, 1), (11, 1)]\n"
     ]
    }
   ],
   "source": [
    "for vector in corpus_memory_friendly:\n",
    "    print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，用来构建词典，而不用把整个数据加载到内存。然后，我们将生成停用词的token id，为了将其从dictionary中删除；以及从词频词典中只出现一次的词的token id。最后，我们把这些从dictionary中过滤掉。\n",
    "\n",
    "要注意的是，像dictionary.filter_tokens（或者其他一些方法比如dictionary.add_document）会调用dictionary.compactify()来消除id的gap，所以每个词的token id会有变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-06 17:21:54,434 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-11-06 17:21:54,437 : INFO : built Dictionary(46 unique tokens: [u'and', u'paths', u'minors', u'generation', u'testing']...) from 9 documents (total 69 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(10 unique tokens: [u'minors', u'graph', u'trees,', u'system', u'computer']...)\n"
     ]
    }
   ],
   "source": [
    "from six import iteritems\n",
    "\n",
    "#收集所有token的统计信息\n",
    "dictionary2 = corpora.Dictionary(line.lower().split() for line in open('/home/david/code/jupyter/gensim/mycorpus.txt'))\n",
    "\n",
    "#找到停用词和只出现一次的词的id\n",
    "stop_ids = [ dictionary2.token2id[stopword] for stopword in stoplist if stopword in dictionary2.token2id ]\n",
    "once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary2.dfs) if docfreq == 1 ]\n",
    "\n",
    "#删除上述id\n",
    "dictionary2.filter_tokens(stop_ids + once_ids)\n",
    "print(dictionary2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Corpus格式化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了将向量空间的corpus保存到本地，有几种文件格式。Gemsim通过之前提到的流处理接口来实现。\n",
    "\n",
    "**保存模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-06 17:31:13,877 : INFO : storing corpus in Matrix Market format to /tmp/corpus.mm\n",
      "2017-11-06 17:31:13,882 : INFO : saving sparse matrix to /tmp/corpus.mm\n",
      "2017-11-06 17:31:13,883 : INFO : PROGRESS: saving document #0\n",
      "2017-11-06 17:31:13,884 : INFO : saved 2x2 matrix, density=25.000% (1/4)\n",
      "2017-11-06 17:31:13,885 : INFO : saving MmCorpus index to /tmp/corpus.mm.index\n"
     ]
    }
   ],
   "source": [
    "#创建一个corpus\n",
    "corpus = [[(1,0.5)], []]\n",
    "corpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'corpus.mm'), corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**加载模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-06 17:33:27,324 : INFO : loaded corpus index from /tmp/corpus.mm.index\n",
      "2017-11-06 17:33:27,327 : INFO : initializing corpus reader from /tmp/corpus.mm\n",
      "2017-11-06 17:33:27,328 : INFO : accepted corpus with 2 documents, 2 features, 1 non-zero entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(2 documents, 2 features, 1 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "corpus = corpora.MmCorpus(os.path.join(TEMP_FOLDER, 'corpus.mm'))\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(1, 0.5)], []]\n"
     ]
    }
   ],
   "source": [
    "print(list(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.5)]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#也可以遍历输出\n",
    "for doc in corpus:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 与Numpy和Scipy互通\n",
    "\n",
    "略"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
