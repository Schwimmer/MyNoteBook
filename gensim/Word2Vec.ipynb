{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onclick=\"$('.output_stderr').toggle();\">Toggle Code</button>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<button onclick=\"$('.output_stderr').toggle();\">Toggle Code</button>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder \"/tmp\" will be used to save temporary dictionary and corpus.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-10 10:57:26,088 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "%run common.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load /home/david/code/nlp/WordSeg.py\n",
    "#!/usr/bin/env python2\n",
    "\"\"\"\n",
    "Created on Thu Nov  9 10:00:53 2017\n",
    "\n",
    "@author: david\n",
    "\"\"\"\n",
    "\n",
    "from jieba import posseg\n",
    "import jieba\n",
    "import re\n",
    "import sys\n",
    "import codecs\n",
    "import os\n",
    "from gensim import corpora, models, similarities\n",
    "import multiprocessing\n",
    "\n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "'''\n",
    "使用结巴进行分词，可以加载自定义词典和停用词词典，可以过滤非中文字符\n",
    "segMode :\n",
    "e -- 精确模式，试图将句子最精确地切开，适合文本分析，也是默认的模式\n",
    "a -- 全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义\n",
    "s -- 搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词\n",
    "userDictPath：用户自定义词典\n",
    "stopWordPath：停用词词典\n",
    "isFilterNoChinese：是否过滤非中文字符，默认过滤\n",
    "'''\n",
    "class WordSeg(object):\n",
    "    def __init__(self, segMode, userDictPath = None, stopWordPath = None, isFilterNoChinese = True):\n",
    "        self._segMode = segMode\n",
    "        self._userDictPath = userDictPath\n",
    "        self._stopWordPath = stopWordPath\n",
    "        self._isFilterNoChinese = isFilterNoChinese\n",
    "        \n",
    "    def filterNoChinese(self, segStr):\n",
    "        pattern = re.compile(ur'[^\\u4e00-\\u9fa5]')\n",
    "        #去掉多余的空格 \n",
    "        return re.sub(r\"\\s{2,}\", \"\", \" \".join(pattern.split(segStr)).strip())\n",
    "    \n",
    "    def generateStopWords(self):\n",
    "        stopWords = {}\n",
    "        if not self._stopWordPath == None:\n",
    "            fstop = open(self._stopWordPath, 'r')\n",
    "            for word in fstop:\n",
    "                #解码为unicode进行处理\n",
    "                stopWords[word.strip().decode('utf-8', 'ignore')] = word.strip().decode('utf-8', 'ignore')\n",
    "        return stopWords\n",
    "        \n",
    "    def seg(self, segStr):\n",
    "        segStr = segStr.strip().decode('utf8', 'ignore')\n",
    "        if not self._userDictPath == None:\n",
    "            jieba.load_userdict(self._userDictPath)\n",
    "            \n",
    "        if self._isFilterNoChinese:\n",
    "            segStr = self.filterNoChinese(segStr)\n",
    "        \n",
    "        #多线程分词\n",
    "#        jieba.enable_parallel(5)\n",
    "        wordGenList = []\n",
    "        if self._segMode == 'a':\n",
    "            wordGenList = [word for word in jieba.cut(segStr, cut_all=True) if word not in self.generateStopWords()]\n",
    "        elif self._segMode == 's':\n",
    "            wordGenList = [word for word in jieba.cut_for_search(segStr) if word not in self.generateStopWords()]\n",
    "        else:\n",
    "            wordGenList = [word for word in jieba.cut(segStr, cut_all=False) if word not in self.generateStopWords()]\n",
    "        \n",
    "        return wordGenList\n",
    "    \n",
    "    def posSeg(self, segStr):\n",
    "        segStr = segStr.strip().decode('utf8', 'ignore')\n",
    "        if not self._userDictPath == None:\n",
    "            jieba.load_userdict(self._userDictPath)\n",
    "            \n",
    "        if self._isFilterNoChinese:\n",
    "            segStr = self.filterNoChinese(segStr)\n",
    "        wordPosGenList = posseg.cut(segStr, HMM=True)\n",
    "        return wordPosGenList\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FOLDER_DIR = '/home/david/code/kuandai/'\n",
    "#设置了一大一小两个语料集，为了测试增量训练\n",
    "smallRawTextPath = os.path.join(FOLDER_DIR, 'smallRaw.txt')\n",
    "bigRawTextPath = os.path.join(FOLDER_DIR, 'bigRaw.txt')\n",
    "\n",
    "smallSegTextPath = os.path.join(FOLDER_DIR, 'smallSeg.txt')\n",
    "bigSegTextPath = os.path.join(FOLDER_DIR, 'bigSeg.txt')\n",
    "\n",
    "smallBinModelPath = os.path.join(FOLDER_DIR, 'small.model.bin')\n",
    "smallTxtModelPath = os.path.join(FOLDER_DIR, 'small.model.txt')\n",
    "\n",
    "bigBinModelPath = os.path.join(FOLDER_DIR, 'big.model.bin')\n",
    "bigTxtModelPath = os.path.join(FOLDER_DIR, 'big.model.txt')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class gensim.models.word2vec.Word2Vec(sentences=None,size=100,alpha=0.025,window=5, min_count=5, max_vocab_size=None, sample=0.001,seed=1, workers=3,min_alpha=0.0001, sg=0, hs=0, negative=5, cbow_mean=1, hashfxn=<built-in function hash>,iter=5,null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000)\n",
    "参数：\n",
    "·  sentences：可以是一个list，对于大语料集，建议使用BrownCorpus,Text8Corpus或lineSentence构建。\n",
    "·  sg： 用于设置训练算法，默认为0，对应CBOW算法；sg=1则采用skip-gram算法。\n",
    "·  size：是指特征向量的维度，默认为100。大的size需要更多的训练数据,但是效果会更好. 推荐值为几十到几百。\n",
    "·  window：表示当前词与预测词在一个句子中的最大距离是多少\n",
    "·  alpha: 是学习速率\n",
    "·  seed：用于随机数发生器。与初始化词向量有关。\n",
    "·  min_count: 可以对字典做截断. 词频少于min_count次数的单词会被丢弃掉, 默认值为5\n",
    "·  max_vocab_size: 设置词向量构建期间的RAM限制。如果所有独立单词个数超过这个，则就消除掉其中最不频繁的一个。每一千万个单词需要大约1GB的RAM。设置成None则没有限制。\n",
    "·  sample: 高频词汇的随机降采样的配置阈值，默认为1e-3，范围是(0,1e-5)\n",
    "·  workers参数控制训练的并行数。\n",
    "·  hs: 如果为1则会采用hierarchica·softmax技巧。如果设置为0（defau·t），则negative sampling会被使用。\n",
    "·  negative: 如果>0,则会采用negativesamp·ing，用于设置多少个noise words\n",
    "·  cbow_mean: 如果为0，则采用上下文词向量的和，如果为1（defau·t）则采用均值。只有使用CBOW的时候才起作用。\n",
    "·  hashfxn： hash函数来初始化权重。默认使用python的hash函数\n",
    "·  iter： 迭代次数，默认为5\n",
    "·  trim_rule： 用于设置词汇表的整理规则，指定那些单词要留下，哪些要被删除。可以设置为None（min_count会被使用）或者一个接受()并返回RU·E_DISCARD,uti·s.RU·E_KEEP或者uti·s.RU·E_DEFAU·T的函数。\n",
    "·  sorted_vocab： 如果为1（defau·t），则在分配word index 的时候会先对单词基于频率降序排序。\n",
    "·  batch_words：每一批的传递给线程的单词的数量，默认为10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seg(inpath, outpath):\n",
    "    wordseg = WordSeg('e', stopWordPath='/home/david/code/data/nlp/stopword.txt')\n",
    "    #读进来是utf8的编码\n",
    "    infile = codecs.open(inpath, 'r', encoding='utf8')\n",
    "    outfile = codecs.open(outpath, 'w', encoding='utf8')\n",
    "    \n",
    "    #对每一行分词并输出到文件\n",
    "    line = infile.readline()\n",
    "    while line:\n",
    "        segWords = wordseg.seg(line)\n",
    "        outfile.writelines(' '.join(segWords))\n",
    "        outfile.writelines('\\n')\n",
    "        line = infile.readline()\n",
    "    \n",
    "    infile.close()\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(inPath, binModelPath, TxtModelPath):\n",
    "    '''\n",
    "    LineSentence\n",
    "    size：向量长度\n",
    "    min_count：词频，低于该词频的会被过滤\n",
    "    window：看前后几个词\n",
    "    workers：设置并发训练时候的线程数\n",
    "    '''\n",
    "    #用gensim自带的方法读取文件路径来训练\n",
    "#    model = models.Word2Vec(models.word2vec.LineSentence(inPath), size=20, window=2, min_count=1, workers=multiprocessing.cpu_count())\n",
    "    #用yield读文件，将读取的文件内容传给gensim\n",
    "    model = models.Word2Vec(inPath, size=20, window=2, min_count=1, workers=multiprocessing.cpu_count())\n",
    "    \n",
    "    model.save(binModelPath)\n",
    "    model.wv.save_word2vec_format(TxtModelPath, binary=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
